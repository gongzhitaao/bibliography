#+TITLE: Notes on: Nowlan, S. J., & Hinton, G. E. (1992): Simplifying Neural Networks By Soft Weight-Sharing

* Gist

mixture of a *narrow* Gaussian and a *broad* Gaussian.

Assume the weight values are generated from a Gaussian mixture, the conditional
probability that a particular weight, \(w_i\), is generated by a particular
gaussian, \(j\), is called the /responsibility/ of that gaussian for the weight
and is given by

\[r_j(w_i) = \frac{\pi_j p_j(w_i)}{\sum_k \pi_k p_k(w_i)}\]

The cost function is defined as

\[C = \frac{K}{\sigma_y^2}\sum_c \frac{1}{2}(y_c - d_c)^2 + \sum_i \log \sum_j
\pi_j p_j(w_i)\]

weights are initialized uniformly \([-a, a]\), means of Gaussian are initialized
*evenly* in \([-a, a]\) with variances equal to the spacing between adjacent
means.
