#+TITLE: Notes on: Kawaguchi, K. (2016): Deep learning without poor local minima

* Gist

The squared loss function of deep linear neural networks with any
depth and any widths:

1. the function is non-convex and non-concave
2. every local minimum is a global minimum
3. every critical point that is not a global minimum is a saddle point
4. there exist "bad" saddle points (where the Hessian has no negative
   eigenvalue) for the deeper networks (with more than three layers),
   whereas there is no bad saddle point for the shallow networks (with
   three layers).

* References

- [ ] cite:baldi1988-linear
- [ ] cite:baldi1989-neural
- [ ] cite:baldi2012-complex
- [ ] cite:choromanska2014-loss
- [ ] cite:choromanska2015-open
- [ ] cite:ge2015-escaping
